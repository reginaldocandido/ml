import streamlit as st
import google.generativeai as genai
import time

# --- Configura√ß√£o da P√°gina ---
st.set_page_config(
    page_title="Decodificador de G√≠rias üßê",
    page_icon="üßê",
    layout="centered",
)

# --- Chamada da API e Configura√ß√£o do Modelo ---

# Fun√ß√£o para carregar o modelo de forma segura
# O @st.cache_resource garante que o modelo seja carregado apenas uma vez.
@st.cache_resource
def load_model():
    """
    Carrega o modelo generativo do Gemini.
    Levanta uma exce√ß√£o se a API key n√£o estiver configurada nos secrets do Streamlit.
    """
    if "GOOGLE_API_KEY" not in st.secrets:
        st.error("Erro: GOOGLE_API_KEY n√£o encontrada nos secrets do Streamlit.")
        st.caption("Por favor, adicione sua chave da API do Google AI Studio aos 'Secrets' do seu app no Streamlit Community Cloud.")
        st.stop() # Para a execu√ß√£o se a chave n√£o estiver presente

    try:
        genai.configure(api_key=st.secrets["GOOGLE_API_KEY"])
        
        # Configura√ß√µes de seguran√ßa para o modelo
        safety_settings = [
            {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
        ]
        
        # Configura√ß√µes de gera√ß√£o (criatividade vs. precis√£o)
        generation_config = {
            "temperature": 0.7,
            "top_p": 1,
            "top_k": 1,
            "max_output_tokens": 512,
        }
        
        model = genai.GenerativeModel(
            model_name="gemini-2.5-flash-preview-09-2025",
            generation_config=generation_config,
            safety_settings=safety_settings
        )
        return model
    except Exception as e:
        st.error(f"Erro ao inicializar o modelo: {e}")
        st.stop()

# --- Construtor do Prompt ---
def build_prompt(giria, publico_alvo):
    """
    Cria o prompt formatado para enviar ao modelo Gemini.
    """
    # Persona do LLM
    prompt = f"""
    Voc√™ √© o "Decodificador de G√≠rias", um especialista em cultura da internet e lingu√≠stica moderna. 
    Sua tarefa √© explicar uma g√≠ria de forma clara, concisa e adaptada ao p√∫blico-alvo.

    **G√≠ria a ser explicada:** "{giria}"
    
    **P√∫blico-alvo da explica√ß√£o:** "{publico_alvo}"

    **Formato da Resposta (Obrigat√≥rio):**
    1.  **Defini√ß√£o:** Comece com uma defini√ß√£o direta (O que significa?).
    2.  **Origem/Contexto:** (Se souber) Explique brevemente de onde veio (jogo, rede social, etc.).
    3.  **Exemplo de Uso:** D√™ 1 ou 2 frases de exemplo.
    
    Adapte o tom da explica√ß√£o para o p√∫blico-alvo solicitado.
    """
    return prompt

# --- Interface do Streamlit (UI) ---

# Carrega o modelo
try:
    model = load_model()
except Exception as e:
    st.error(f"N√£o foi poss√≠vel carregar a aplica√ß√£o. Erro: {e}")
    # Se o load_model() falhar (ex: API key), ele j√° ter√° mostrado o erro,
    # mas garantimos que a UI n√£o tente ser renderizada.
    st.stop()


# T√≠tulo e Subt√≠tulo
st.title("Decodificador de G√≠rias üßê")
st.markdown("N√£o entendeu o que o 'cria' falou? Ou o que significa 'rizz'? Deixa que eu traduzo!")

# --- Formul√°rio de Inputs ---
with st.form("giria_form"):
    giria_input = st.text_input(
        "Qual g√≠ria voc√™ quer entender?",
        placeholder="Ex: tankar, cringe, 'meter o shape', rizz..."
    )
    
    publico_alvo_input = st.selectbox(
        "Como voc√™ quer a explica√ß√£o?",
        [
            "Para meus pais (bem simples e did√°tico)",
            "Para um colega de trabalho (tom casual, mas profissional)",
            "Para um amigo (descontra√≠do)",
            "T√©cnica (etimologia e contexto cultural)"
        ]
    )
    
    submitted = st.form_submit_button("Decodificar!")

# --- L√≥gica de Execu√ß√£o ---
if submitted:
    if not giria_input:
        st.warning("Por favor, digite uma g√≠ria para decodificar.")
    else:
        # 1. Construir o prompt
        prompt_final = build_prompt(giria_input, publico_alvo_input)
        
        # 2. Chamar a API com spinner (indicador de carregamento)
        with st.spinner(f"Decodificando '{giria_input}'... üß†"):
            try:
                # 3. Gerar a resposta
                response = model.generate_content(prompt_final)
                
                # 4. Mostrar a resposta
                st.divider()
                st.subheader(f"Aqui est√° a decodifica√ß√£o de '{giria_input}':")
                st.markdown(response.text)
                
            except Exception as e:
                st.error(f"Houve um problema ao contatar a IA: {e}")
                st.caption("Isso pode ser um problema tempor√°rio na API. Tente novamente em alguns segundos.")

st.divider()
st.caption("Um projeto de exemplo constru√≠do com Python, Streamlit e a API do Gemini.")
